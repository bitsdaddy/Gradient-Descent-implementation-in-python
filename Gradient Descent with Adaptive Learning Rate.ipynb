{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72501dd0",
   "metadata": {},
   "source": [
    "# Gradient Descent with Adaptive Learning rate\n",
    "\n",
    "Our modified code now includes an adaptive learning rate mechanism based on the change in gradient direction. This adaptive learning rate function attempts to increase the learning rate if the angle between the new and previous gradient vectors is less than 45 degrees and decrease it otherwise.\n",
    "\n",
    "**1. Adaptive Learning Rate Function (`adaptive_learning_rate`):**\n",
    "   - This function calculates the dot product of the new and previous gradients and compares it with a threshold to determine whether the angle between the vectors is less than 45 degrees.\n",
    "   - If the angle is less than 45 degrees, it increases the learning rate by a factor of 1.25. Otherwise, it decreases the learning rate by a factor of 0.9.\n",
    "\n",
    "**2. Integration of Adaptive Learning Rate in `descent` Function:**\n",
    "   - The `adaptive_learning_rate` function is called within the main optimization loop to adjust the learning rate for each iteration.\n",
    "\n",
    "**3. Additional Information in Print Statements:**\n",
    "   - The learning rate is now printed in each iteration for better insight into the adaptive process.\n",
    "\n",
    "**4. Minor Adjustments:**\n",
    "   - A line break is added after printing the initial objective value for better readability.\n",
    "\n",
    "The overall structure of the code remains intact, and the adaptive learning rate mechanism is now incorporated. We can run this modified code to observe how the learning rate adapts during the optimization process.\n",
    "We have experimented with multiple values of adaptive learning threshold and the factors by which the learning rate is increased or decreased and concluded that these values are best for this particular objective function. For other problems we can experiment with other values and see which better suits our requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08cc3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Parameters: [5, 10]\n",
      "Initial Objective Value: 457 \n",
      "\n",
      "Iteration: 1\n",
      "Learning rate:  0.125\n",
      "Updated Parameters: [1.875, 0.0]\n",
      "Objective Value: 8.171875 \n",
      "\n",
      "Iteration: 2\n",
      "Learning rate:  0.15625\n",
      "Updated Parameters: [0.8984375, 0.0]\n",
      "Objective Value: 4.92938232421875 \n",
      "\n",
      "Iteration: 3\n",
      "Learning rate:  0.1953125\n",
      "Updated Parameters: [0.8221435546875, 0.0]\n",
      "Objective Value: 4.917042300105095 \n",
      "\n",
      "Iteration: 4\n",
      "Learning rate:  0.17578125\n",
      "Updated Parameters: [0.8339452743530273, 0.0]\n",
      "Objective Value: 4.916667790082101 \n",
      "\n",
      "Iteration: 5\n",
      "Learning rate:  0.158203125\n",
      "Updated Parameters: [0.8333644084632397, 0.0]\n",
      "Objective Value: 4.916666669563657 \n",
      "\n",
      "Iteration: 6\n",
      "Learning rate:  0.1423828125\n",
      "Updated Parameters: [0.8333378610768705, 0.0]\n",
      "Objective Value: 4.916666666728168 \n",
      "\n",
      "Iteration: 7\n",
      "Learning rate:  0.12814453125000003\n",
      "Updated Parameters: [0.8333343798434314, 0.0]\n",
      "Objective Value: 4.916666666669952 \n",
      "\n",
      "Iteration: 8\n",
      "Learning rate:  0.11533007812500003\n",
      "Updated Parameters: [0.8333336556788832, 0.0]\n",
      "Objective Value: 4.916666666666979 \n",
      "\n",
      "Converged after 8 iterations.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def F(w):\n",
    "    \"\"\"Objective function.\"\"\"\n",
    "    return 3 * w[0]**2 + 4 * w[1]**2 - 5 * w[0] + 7\n",
    "\n",
    "def grad(w):\n",
    "    \"\"\"Gradient of the objective function.\"\"\"\n",
    "    g = [0] * 2\n",
    "    g[0] = 6 * w[0] - 5\n",
    "    g[1] = 8 * w[1]\n",
    "    return g\n",
    "\n",
    "def has_converged(w_new, w_prev, threshold):\n",
    "    \"\"\"Check if the parameters have converged.\"\"\"\n",
    "    return np.linalg.norm(np.array(w_new, dtype=float) - np.array(w_prev, dtype=float)) < threshold\n",
    "\n",
    "def adaptive_learning_rate(lr, grad_prev, grad_new):\n",
    "    \"\"\"Adapt the learning rate based on the change in gradient.\"\"\"\n",
    "    if np.dot(grad_new, grad_prev) > (1/math. sqrt(2)):\n",
    "        \"\"\"If angle between new gradient and previous gradient vectors is less than 45 degrees increase learning rate\"\"\"\n",
    "        return lr * 1.25\n",
    "    else:\n",
    "        \"\"\"If angle between new gradient and previous gradient vectors is greater than 45 degrees decrease learning rate\"\"\"\n",
    "        return lr * 0.9\n",
    "\n",
    "def descent(w_new, w_prev, lr, threshold, max_iter=1000):\n",
    "    \"\"\"Gradient Descent optimization with adaptive learning rate.\"\"\"\n",
    "    print(\"Initial Parameters:\", w_prev)\n",
    "    print(\"Initial Objective Value:\", F(w_prev),\"\\n\")\n",
    "    \n",
    "    grad_prev = grad(w_prev)\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        w_prev = w_new\n",
    "        grad_new = grad(w_prev)\n",
    "        \n",
    "        # Update the learning rate adaptively\n",
    "        lr = adaptive_learning_rate(lr, grad_prev, grad_new)\n",
    "        \n",
    "        w_0 = w_prev[0] - lr * grad_new[0]\n",
    "        w_1 = w_prev[1] - lr * grad_new[1]\n",
    "        w_new = [w_0, w_1]\n",
    "        \n",
    "        print(\"Iteration:\", iteration + 1)\n",
    "        print(\"Learning rate: \",lr)\n",
    "        print(\"Updated Parameters:\", w_new)\n",
    "        print(\"Objective Value:\", F(w_new),\"\\n\")\n",
    "        \n",
    "        # Check if the objective function is increasing\n",
    "        if F(w_new) > F(w_prev):\n",
    "            print(\"Objective function is increasing. Try reducing the learning rate.\")\n",
    "            break\n",
    "\n",
    "        # Check for convergence\n",
    "        if has_converged(w_new, w_prev, threshold):\n",
    "            print(f\"Converged after {iteration+1} iterations.\")\n",
    "            break\n",
    "        \n",
    "        # Check for divergence (NaN values)\n",
    "        if any(math.isnan(val) for val in [F(w_new)] + grad_new):\n",
    "            print(\"Divergence detected. Try reducing the learning rate.\")\n",
    "            break\n",
    "        \n",
    "        # Update the gradient for the next iteration\n",
    "        grad_prev = grad_new\n",
    "\n",
    "# Example usage with adaptive learning rate\n",
    "descent([5, 10], [5, 10], 0.1, pow(10, -6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c7ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
