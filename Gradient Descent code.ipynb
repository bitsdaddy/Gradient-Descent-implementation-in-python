{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee475ac",
   "metadata": {},
   "source": [
    "# Gradient Descent in Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb4e42",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Gradient descent is a widely used iterative optimization algorithm employed in various machine learning and optimization tasks. Its core objective is to minimize a given cost function by iteratively adjusting parameters in the direction of the steepest decrease in the function. Typically applied to minimize a cost function, such as squared error, the goal of gradient descent is to find parameter values that yield the minimum cost. Starting with initial values (e.g., \\(w=0, b=0\\)), the algorithm updates these parameters to reduce the cost function (\\(J(w, b)\\)) until reaching a local minimum. It's important to note that there may be multiple local minima, akin to navigating through a hilly terrain with varying heights and depths. The choice of the starting point influences the path taken, and the x and y axes represent the parameter space (e.g., \\(W\\) and \\(B\\)), while the cost function value reflects the height level being minimized. The implementation provided here captures the fundamental definition and purpose of gradient descent.\n",
    "\n",
    "## How Gradient Descent Works\n",
    "\n",
    "1. **Objective Function:**\n",
    "   - The optimization problem begins with defining an objective function that needs to be minimized or maximized. In the context of machine learning, this function often represents a measure of error or a cost associated with the model's predictions.\n",
    "\n",
    "2. **Gradient Calculation:**\n",
    "   - The gradient of the objective function is computed with respect to the parameters. The gradient points in the direction of the steepest ascent, and its negative points in the direction of the steepest descent.\n",
    "\n",
    "3. **Parameter Update:**\n",
    "   - The parameters are updated iteratively by moving in the opposite direction of the gradient. The learning rate determines the step size for each update.\n",
    "\n",
    "4. **Convergence:**\n",
    "   - The process continues until a convergence criterion is met, such as the change in parameters falling below a specified threshold.\n",
    "\n",
    "## Overview of the Implemented Code\n",
    "\n",
    "In the project, a simple implementation of gradient descent has been developed. The code includes the following components:\n",
    "\n",
    "- **Objective Function (`F`):**\n",
    "  - The specific objective function being minimized is \\(3w_0^2 + 4w_1^2 - 5w_0 + 7\\).\n",
    "\n",
    "- **Gradient Function (`grad`):**\n",
    "  - The gradient of the objective function is computed to guide the parameter updates.\n",
    "\n",
    "- **Convergence Check (`has_converged`):**\n",
    "  - A convergence check ensures that the optimization stops when parameters are sufficiently close to the optimal values.\n",
    "\n",
    "- **Gradient Descent Function (`descent`):**\n",
    "  - The main optimization routine that iteratively updates parameters and checks for convergence or divergence.\n",
    "\n",
    "## Potential Extensions\n",
    "\n",
    "- **Learning Rate Adaptation:**\n",
    "  - Consider implementing learning rate schedules or adaptive learning rate methods to enhance convergence.\n",
    "\n",
    "- **Visualization:**\n",
    "  - Visualizing the optimization process, such as plotting the objective function over iterations, can provide valuable insights.\n",
    "\n",
    "- **Documentation:**\n",
    "  - Comments and documentation within the code can enhance readability and facilitate understanding.\n",
    "\n",
    "This gradient descent implementation serves as a foundational step in understanding optimization algorithms, and its principles can be extended and applied to more complex problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62afbe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Parameters: [5, 10]\n",
      "Initial Objective Value: 457\n",
      "Iteration: 1\n",
      "Updated Parameters: [2.5, 2.0]\n",
      "Objective Value: 29.25\n",
      "Iteration: 2\n",
      "Updated Parameters: [1.5, 0.3999999999999999]\n",
      "Objective Value: 6.89\n",
      "Iteration: 3\n",
      "Updated Parameters: [1.1, 0.07999999999999996]\n",
      "Objective Value: 5.155600000000001\n",
      "Iteration: 4\n",
      "Updated Parameters: [0.9400000000000001, 0.015999999999999986]\n",
      "Objective Value: 4.951824\n",
      "Iteration: 5\n",
      "Updated Parameters: [0.876, 0.0031999999999999963]\n",
      "Objective Value: 4.9221689600000005\n",
      "Iteration: 6\n",
      "Updated Parameters: [0.8503999999999999, 0.0006399999999999991]\n",
      "Objective Value: 4.9175421184\n",
      "Iteration: 7\n",
      "Updated Parameters: [0.84016, 0.00012799999999999975]\n",
      "Objective Value: 4.916806542336\n",
      "Iteration: 8\n",
      "Updated Parameters: [0.836064, 2.5599999999999945e-05]\n",
      "Objective Value: 4.91668903890944\n",
      "Iteration: 9\n",
      "Updated Parameters: [0.8344256, 5.119999999999988e-06]\n",
      "Objective Value: 4.916670245910938\n",
      "Iteration: 10\n",
      "Updated Parameters: [0.83377024, 1.0239999999999973e-06]\n",
      "Objective Value: 4.916667239333167\n",
      "Iteration: 11\n",
      "Updated Parameters: [0.833508096, 2.047999999999994e-07]\n",
      "Objective Value: 4.916666758292804\n",
      "Iteration: 12\n",
      "Updated Parameters: [0.8334032384, 4.095999999999986e-08]\n",
      "Objective Value: 4.916666681326828\n",
      "Iteration: 13\n",
      "Updated Parameters: [0.8333612953599999, 8.191999999999972e-09]\n",
      "Objective Value: 4.916666669012292\n",
      "Iteration: 14\n",
      "Updated Parameters: [0.833344518144, 1.6383999999999938e-09]\n",
      "Objective Value: 4.916666667041967\n",
      "Iteration: 15\n",
      "Updated Parameters: [0.8333378072576, 3.276799999999986e-10]\n",
      "Objective Value: 4.916666666726714\n",
      "Iteration: 16\n",
      "Updated Parameters: [0.83333512290304, 6.553599999999972e-11]\n",
      "Objective Value: 4.916666666676274\n",
      "Iteration: 17\n",
      "Updated Parameters: [0.833334049161216, 1.3107199999999942e-11]\n",
      "Objective Value: 4.9166666666682035\n",
      "Iteration: 18\n",
      "Updated Parameters: [0.8333336196644865, 2.6214399999999877e-12]\n",
      "Objective Value: 4.916666666666913\n",
      "Converged after 18 iterations.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def F(w):\n",
    "    \"\"\"Objective function.\"\"\"\n",
    "    return 3 * w[0]**2 + 4 * w[1]**2 - 5 * w[0] + 7\n",
    "\n",
    "def grad(w):\n",
    "    \"\"\"Gradient of the objective function.\"\"\"\n",
    "    g = [0] * 2\n",
    "    g[0] = 6 * w[0] - 5\n",
    "    g[1] = 8 * w[1]\n",
    "    return g\n",
    "\n",
    "def has_converged(w_new, w_prev, threshold):\n",
    "    \"\"\"Check if the parameters have converged.\"\"\"\n",
    "    return np.linalg.norm(np.array(w_new, dtype=float) - np.array(w_prev, dtype=float)) < threshold\n",
    "\n",
    "def descent(w_new, w_prev, lr, threshold, max_iter=1000):\n",
    "    \"\"\"Gradient Descent optimization.\"\"\"\n",
    "    print(\"Initial Parameters:\", w_prev)\n",
    "    print(\"Initial Objective Value:\", F(w_prev))\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        w_prev = w_new\n",
    "        w_0 = w_prev[0] - lr * grad(w_prev)[0]\n",
    "        w_1 = w_prev[1] - lr * grad(w_prev)[1]\n",
    "        w_new = [w_0, w_1]\n",
    "        \n",
    "        print(\"Iteration:\", iteration + 1)\n",
    "        print(\"Updated Parameters:\", w_new)\n",
    "        print(\"Objective Value:\", F(w_new))\n",
    "        \n",
    "        # Check if the objective function is increasing\n",
    "        if F(w_new) > F(w_prev):\n",
    "            print(\"Objective function is increasing. Try reducing the learning rate.\")\n",
    "            break\n",
    "\n",
    "        # Check for convergence\n",
    "        if has_converged(w_new, w_prev, threshold):\n",
    "            print(f\"Converged after {iteration+1} iterations.\")\n",
    "            break\n",
    "        \n",
    "        # Check for divergence (NaN values)\n",
    "        if any(math.isnan(val) for val in [F(w_new)] + grad(w_new)):\n",
    "            print(\"Divergence detected. Try reducing the learning rate.\")\n",
    "            break\n",
    "\n",
    "# Example usage with adaptive learning rate\n",
    "descent([5, 10], [5, 10], 0.1, pow(10, -6))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
